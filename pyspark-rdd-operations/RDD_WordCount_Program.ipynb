{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-gregory",
   "metadata": {},
   "source": [
    "# WordCount Program\n",
    "\n",
    "Follow the below steps:\n",
    "\n",
    "- Create SparkContext object.\n",
    "- Use textFile function to read the input file from the local file storage or hdfs\n",
    "- Make sure to filter out empty lines (line length = 0)\n",
    "- Use map transformation to split each line from the RDD.\n",
    "- To this new transformed RDD use a map function that assignes each word with a value of 1.\n",
    "- Chain this transformation with reduceByKey() within which create a lambda function that adds the current value and accumulated value.\n",
    "- Lastly chain the transformations with a sortByKey to get new RDD with sorted key values.\n",
    "- Simply collect the result RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latest-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName('RDD_WordCount').setMaster('local')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "critical-drama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello from pySpark']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile_local = sc.textFile('file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt')\n",
    "textFile_local.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sized-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello from pySpark']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile('hdfs://0.0.0.0:19000/data/sample.txt')\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "intelligent-warner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'from', 'pySpark']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonEmptyLines = textFile.filter(lambda line: len(line) > 0)\n",
    "words = NonEmptyLines.flatMap(lambda lines: lines.split(' '))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "junior-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1), ('from', 1), ('pySpark', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCount = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y).sortByKey()\n",
    "wordCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-revelation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
