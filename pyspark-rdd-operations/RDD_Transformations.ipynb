{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-poker",
   "metadata": {},
   "source": [
    "# First RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-basin",
   "metadata": {},
   "source": [
    "Importing SparkConf (configuration) and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-portsmouth",
   "metadata": {},
   "source": [
    "Defining a configuration as conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "perceived-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"RDD_TRANSFORMATIONS_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-invalid",
   "metadata": {},
   "source": [
    "Creating a spark context sc and passing the configuration created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-symphony",
   "metadata": {},
   "source": [
    "Taking a simple data collection (List) and creating a rdd by parallelizing the list. After that applying take() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-tolerance",
   "metadata": {},
   "source": [
    "# Reading from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appreciated-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello from pySpark']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-cargo",
   "metadata": {},
   "source": [
    "# Persistance and Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-pickup",
   "metadata": {},
   "source": [
    "### RDD Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wired-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddNums = sc.parallelize(range(1, 1000, 2))\n",
    "oddNums.take(10)\n",
    "oddNums.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-essay",
   "metadata": {},
   "source": [
    "### RDD Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-reynolds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.textFile(\"file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt\")\n",
    "rdd2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-laser",
   "metadata": {},
   "source": [
    "# Spark Cache vs Persist\n",
    "\n",
    "Using <strong>cache()</strong> and <strong>persist()</strong> methods, Spark provides an optimization mechanism to store the intermediate computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation result’s).\n",
    "\n",
    "Both caching and persisting are used to save the Spark RDD, Dataframe and Dataset’s. But, the difference is, <strong>RDD cache()</strong> method default saves it to memory (MEMORY_ONLY) whereas <strong>persist()</strong> method is used to store it to user-defined storage level.\n",
    "\n",
    "When you persist a dataset, each node stores it’s partitioned data in memory and reuses them in other actions on that dataset. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-topic",
   "metadata": {},
   "source": [
    "# Operations of RDDs\n",
    "\n",
    "There are two type of data operations we can perform on an RDD, <strong>transformations</strong> and <strong>actions</strong>\n",
    "- <strong>Transformations</strong> : will return a new RDD as RDDs are generally <strong>Immutable</strong>.\n",
    "- <strong>Actions</strong> : will return a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-weight",
   "metadata": {},
   "source": [
    "## Transformations on RDDs\n",
    "\n",
    "Transformations are lazy operations on RDD that create one or more new RDDs.\n",
    "\n",
    "RDD transformations return a pointer to the new RDD and allow use to create dependencies between RDDs. Each RDD in dependency chain has a function to calculate data and a pointer to the parent RDD.\n",
    "\n",
    "Spark is lazy, so nothing will be executed until we call a transformation or an action that will trigger the job creation and execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-writer",
   "metadata": {},
   "source": [
    "## Map Transformation\n",
    "\n",
    "Passes each element through a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mounted-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 1), ('grapes', 1), ('banana', 1), ('orange', 1), ('kiwi', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([\"apple\", \"grapes\", \"banana\", \"orange\", \"kiwi\"])\n",
    "y = x.map(lambda x: (x, 1))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-course",
   "metadata": {},
   "source": [
    "## FlatMap Transformation\n",
    "\n",
    "Its similar to map transformation, but here each item can be mapped with 0 or more output items, so the function should return a sequence rather than a single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "laughing-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([2, 3, 4])\n",
    "result = rdd3.flatMap(lambda x: range(1, x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surface-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 2), range(1, 3), range(1, 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([2, 3, 4])\n",
    "result = rdd3.map(lambda x: range(1, x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-stanford",
   "metadata": {},
   "source": [
    "## Filter Transformation\n",
    "\n",
    "Filtering rows based on some condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-tablet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = sc.parallelize(range(1, 6))\n",
    "rdd4.filter(lambda x: x%2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-wages",
   "metadata": {},
   "source": [
    "## Sample Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civil-replication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 19, 23, 25, 28, 28, 35, 37, 47, 56, 64, 66, 77, 87, 94, 95, 100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = sc.parallelize(range(1, 101))\n",
    "rdd5.sample(True, .2, seed=42).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "committed-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 5,\n",
       " 11,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 42,\n",
       " 43,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 49,\n",
       " 53,\n",
       " 57,\n",
       " 67,\n",
       " 79,\n",
       " 80,\n",
       " 82,\n",
       " 85,\n",
       " 87,\n",
       " 93,\n",
       " 95,\n",
       " 99]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6 = sc.parallelize(range(1, 101))\n",
    "rdd6.sample(False, .2, seed=42).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-integral",
   "metadata": {},
   "source": [
    "## Union Transformation\n",
    "\n",
    "Combines all the elements of the both rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "olive-survivor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = sc.parallelize(range(1, 10))\n",
    "rdd7_un = sc.parallelize(range(5, 10))\n",
    "rdd7.union(rdd7_un).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-extraction",
   "metadata": {},
   "source": [
    "## Intersection Transformation\n",
    "\n",
    "Getting the common elements from the rdds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "social-worst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = sc.parallelize(range(1, 10))\n",
    "rdd8_inter = sc.parallelize(range(5, 10))\n",
    "sorted(rdd8.intersection(rdd8_inter).collect()) # we are using sorted() to get sorted array in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-skirt",
   "metadata": {},
   "source": [
    "## Distinct Transformation\n",
    "\n",
    "creates new rdd having distinct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serious-classics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9 = sc.parallelize(range(1, 10))\n",
    "rdd9_un = sc.parallelize(range(5, 10))\n",
    "rdd9.union(rdd9_un).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "compressed-marshall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1, 9, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9 = sc.parallelize(range(1, 10))\n",
    "rdd9_un = sc.parallelize(range(5, 10))\n",
    "rdd9.union(rdd9_un).distinct().collect() # gets the distinct from the union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-quest",
   "metadata": {},
   "source": [
    "## SortBy Transformation\n",
    "returns a sorted rdd.\n",
    "\n",
    "- True sorts by ascending order.\n",
    "- False sorts by descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "medical-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 7, 10, 20]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd10 = sc.parallelize([1, 7, 10, 3, 2, 20])\n",
    "rdd10.sortBy(lambda c:c, True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-intensity",
   "metadata": {},
   "source": [
    "## MapPartitions Transformation\n",
    "\n",
    "Similar to map() transformation, i.e. applies function to each row / record\n",
    "\n",
    "### map() vs mapPartition()\n",
    "\n",
    "- <strong>map()</strong> : Spark ```map()``` transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.\n",
    "- <strong>mapPartitions()</strong> : This is exactly the same as map(); the difference being, Spark mapPartitions() provides a facility to do heavy initializations (for example Database connection) once for each partition instead of doing it on every DataFrame row. This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.\n",
    "\n",
    "### PS (important)\n",
    "\n",
    "Parallelize Syntax:\n",
    "```\n",
    "parallelize([collection], number_of_partitions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "direct-religious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11 = sc.parallelize([1, 2, 3, 4], 1)\n",
    "def f(iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "rdd11.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "local-motion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-prize",
   "metadata": {},
   "source": [
    "## MapPartitionsWithIndex\n",
    "\n",
    "applies function to all the partitions in the rdd but also maintains indexes for the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advised-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd12 = sc.parallelize([1, 2, 3, 4])\n",
    "def f(index, iterator): yield index\n",
    "rdd12.mapPartitionsWithIndex(f).sum() ## sum of the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-white",
   "metadata": {},
   "source": [
    "## GroupBy\n",
    "\n",
    "returns new rdd by grouping objects in existing rdd using given groupby key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "light-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd13 = sc.parallelize(range(1, 11))\n",
    "results = rdd13.groupBy(lambda x: x%2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pacific-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 4, 6, 8, 10]), (1, [1, 3, 5, 7, 9])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, sorted(y)) for (x, y) in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sharing-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x25c4529b280>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x25c452398b0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, y) for (x, y) in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-peeing",
   "metadata": {},
   "source": [
    "Here 0, 1 are the indexes of the two groups formed by groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "about-sapphire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6, 8, 10], [1, 3, 5, 7, 9]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(sorted(y)) for (x, y) in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-artwork",
   "metadata": {},
   "source": [
    "## Zip Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suitable-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd14 = sc.parallelize(range(1, 10))\n",
    "rdd14_new = sc.parallelize(range(20, 30))\n",
    "\n",
    "result = sc.parallelize(zip(range(1, 10), range(20, 40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial-drinking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 20),\n",
       " (2, 21),\n",
       " (3, 22),\n",
       " (4, 23),\n",
       " (5, 24),\n",
       " (6, 25),\n",
       " (7, 26),\n",
       " (8, 27),\n",
       " (9, 28)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "opposite-scenario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a','b','c','d'], 1).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "administrative-tamil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a','b','c','d'], 3).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-resort",
   "metadata": {},
   "source": [
    "## Repartition Transformation\n",
    "\n",
    "- used to increase or decrease partitions in an RDD.\n",
    "- does a full shuffle and creates new partition with the data that is distributed evenly.\n",
    "\n",
    "```\n",
    "glom() flattens elements on the same partition\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "funded-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd15 = sc.parallelize(range(1, 20), 4)\n",
    "rdd15.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brutal-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd15.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dedicated-filling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd15.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "accomplished-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd15.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prescription-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partition :  4\n",
      "After Repartition :  2\n"
     ]
    }
   ],
   "source": [
    "# Length of partition\n",
    "print('Initial Partition : ', len(rdd15.glom().collect()))\n",
    "print('After Repartition : ', len(rdd15.repartition(2).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-dallas",
   "metadata": {},
   "source": [
    "## Coalesce Transformation\n",
    "\n",
    "used to reduce the number of partitions in an RDD. Usually we used it when working with large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dominican-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 10), 3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "trained-berry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 10), 3).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-flooring",
   "metadata": {},
   "source": [
    "## Coalesce vs Repartition\n",
    "\n",
    "### Coalesce\n",
    "\n",
    "- uses the existing partitions to minimize the amount of data that's shuffled.\n",
    "- results in partitions with different amounts of data (at times with much different sizes).\n",
    "- Faster\n",
    "\n",
    "### Repartition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-stations",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
