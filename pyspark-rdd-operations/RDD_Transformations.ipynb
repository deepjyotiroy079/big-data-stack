{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-poker",
   "metadata": {},
   "source": [
    "# First RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-basin",
   "metadata": {},
   "source": [
    "Importing SparkConf (configuration) and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-portsmouth",
   "metadata": {},
   "source": [
    "Defining a configuration as conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "perceived-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"RDD_TRANSFORMATIONS_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-invalid",
   "metadata": {},
   "source": [
    "Creating a spark context sc and passing the configuration created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-symphony",
   "metadata": {},
   "source": [
    "Taking a simple data collection (List) and creating a rdd by parallelizing the list. After that applying take() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-tolerance",
   "metadata": {},
   "source": [
    "# Reading from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appreciated-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello from pySpark']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-cargo",
   "metadata": {},
   "source": [
    "# Persistance and Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-pickup",
   "metadata": {},
   "source": [
    "### RDD Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wired-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[24] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddNums = sc.parallelize(range(1, 1000, 2))\n",
    "oddNums.take(10)\n",
    "oddNums.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-essay",
   "metadata": {},
   "source": [
    "### RDD Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distributed-reynolds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt MapPartitionsRDD[26] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.textFile(\"file:///D:/Code/big-data-stack/pyspark-rdd-operations/data/sample.txt\")\n",
    "rdd2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-laser",
   "metadata": {},
   "source": [
    "# Spark Cache vs Persist\n",
    "\n",
    "Using <strong>cache()</strong> and <strong>persist()</strong> methods, Spark provides an optimization mechanism to store the intermediate computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation result’s).\n",
    "\n",
    "Both caching and persisting are used to save the Spark RDD, Dataframe and Dataset’s. But, the difference is, <strong>RDD cache()</strong> method default saves it to memory (MEMORY_ONLY) whereas <strong>persist()</strong> method is used to store it to user-defined storage level.\n",
    "\n",
    "When you persist a dataset, each node stores it’s partitioned data in memory and reuses them in other actions on that dataset. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-topic",
   "metadata": {},
   "source": [
    "# Operations of RDDs\n",
    "\n",
    "There are two type of data operations we can perform on an RDD, <strong>transformations</strong> and <strong>actions</strong>\n",
    "- <strong>Transformations</strong> : will return a new RDD as RDDs are generally <strong>Immutable</strong>.\n",
    "- <strong>Actions</strong> : will return a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-weight",
   "metadata": {},
   "source": [
    "## Transformations on RDDs\n",
    "\n",
    "Transformations are lazy operations on RDD that create one or more new RDDs.\n",
    "\n",
    "RDD transformations return a pointer to the new RDD and allow use to create dependencies between RDDs. Each RDD in dependency chain has a function to calculate data and a pointer to the parent RDD.\n",
    "\n",
    "Spark is lazy, so nothing will be executed until we call a transformation or an action that will trigger the job creation and execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-writer",
   "metadata": {},
   "source": [
    "## Map Transformation\n",
    "\n",
    "Passes each element through a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mounted-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 1), ('grapes', 1), ('banana', 1), ('orange', 1), ('kiwi', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([\"apple\", \"grapes\", \"banana\", \"orange\", \"kiwi\"])\n",
    "y = x.map(lambda x: (x, 1))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-course",
   "metadata": {},
   "source": [
    "## FlatMap Transformation\n",
    "\n",
    "Its similar to map transformation, but here each item can be mapped with 0 or more output items, so the function should return a sequence rather than a single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "laughing-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([2, 3, 4])\n",
    "result = rdd3.flatMap(lambda x: range(1, x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surface-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 2), range(1, 3), range(1, 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([2, 3, 4])\n",
    "result = rdd3.map(lambda x: range(1, x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-stanford",
   "metadata": {},
   "source": [
    "## Filter Transformation\n",
    "\n",
    "Filtering rows based on some condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "attractive-tablet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = sc.parallelize(range(1, 6))\n",
    "rdd4.filter(lambda x: x%2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-purpose",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
