{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fiscal-arbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Name : Lawnmower Man 2: Beyond Cyberspace (1996), Ratings : 21\n",
      "Movie Name : Free Willy 3: The Rescue (1997), Ratings : 27\n",
      "Movie Name : Leave It to Beaver (1997), Ratings : 44\n",
      "Movie Name : Bio-Dome (1996), Ratings : 31\n",
      "Movie Name : Barb Wire (1996), Ratings : 30\n",
      "Movie Name : Crow: City of Angels, The (1996), Ratings : 39\n",
      "Movie Name : Mortal Kombat: Annihilation (1997), Ratings : 43\n",
      "Movie Name : Showgirls (1995), Ratings : 23\n",
      "Movie Name : Grease 2 (1982), Ratings : 24\n",
      "Movie Name : Tales from the Hood (1995), Ratings : 27\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open('u.item', encoding=\"iso-8859-1\") as f:\n",
    "        for lines in f:\n",
    "            fields = lines.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "            \n",
    "    return movieNames\n",
    "\n",
    "\n",
    "def parseInput(lines):\n",
    "    fields = lines.split()\n",
    "    # (movieID, (ratings, 1.0))\n",
    "    return Row(movieId = int(fields[1]), ratings = (float(fields[2])))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creating spark session\n",
    "    spark = SparkSession.builder.appName('Data Frame').getOrCreate()\n",
    "    \n",
    "    # loading the movie names\n",
    "    movieNames = loadMovieNames()\n",
    "    \n",
    "    # creating rdd from u.data file\n",
    "#     lines = sc.textFile('hdfs://localhost:9000/user/deepjyotiroy079/ml-100k/u.data')\n",
    "    lines = spark.sparkContext.textFile('hdfs://localhost:9000/user/deepjyotiroy079/ml-100k/u.data')\n",
    "    \n",
    "    # (movieID, (ratings, 1.0))\n",
    "    movieRatings = lines.map(parseInput)\n",
    "    \n",
    "    movieDataset = spark.createDataFrame(movieRatings)\n",
    "    \n",
    "    averageRatings = movieDataset.groupBy(\"movieID\").avg(\"ratings\")\n",
    "\n",
    "    counts = movieDataset.groupBy(\"movieID\").count()\n",
    "    \n",
    "    averagesAndCounts = counts.join(averageRatings, \"movieID\")\n",
    "    \n",
    "    # using the count property of dataframe to get the total counts of movies\n",
    "    popularAveragesAndCounts = averagesAndCounts.filter('count > 20')\n",
    "    \n",
    "    topTen = popularAveragesAndCounts.orderBy(\"avg(ratings)\").take(10)\n",
    "    \n",
    "    for result in topTen:\n",
    "        print(f'Movie Name : {movieNames[result[0]]}, Ratings : {result[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-league",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
